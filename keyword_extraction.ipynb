{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "import requests\n",
    "import time\n",
    "from IPy import IP\n",
    "from collections import defaultdict\n",
    "import itertools  #used to slice a dictionary\n",
    "\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions used for extraction of text from the url provided\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "def text_from_title(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    title_text = soup.find('title').text\n",
    "    return title_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_domains = [\n",
    "    'www.mail.google.com',\n",
    "    'www.mail.yahoo.com',\n",
    "    'www.account.google.com',\n",
    "    'www.facebook.com',  \n",
    "    'www.docs.google.com'\n",
    "]\n",
    "\n",
    "def isIP(str):\n",
    "    try:\n",
    "        IP(str)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_clean_urls(url_file, clean_url_file):\n",
    "    url_file = open(url_file, 'r')\n",
    "    clean_urls = open(clean_url_file, 'w')\n",
    "    for url in url_file.readlines():\n",
    "        url = url.strip()\n",
    "        spltAr = url.split(\"://\");\n",
    "        protocol = spltAr[0]\n",
    "        if protocol not in ['http', 'https']:\n",
    "            continue\n",
    "        i = (0,1)[len(spltAr)>1];\n",
    "        dm = spltAr[i].split(\"?\")[0].split('/')[0].split(':')[0].lower();\n",
    "        if dm[0:4] != 'www.':\n",
    "            # check if domain is ip address\n",
    "            if isIP(dm):\n",
    "                continue\n",
    "            dm = 'www.' + dm\n",
    "        # domain checks\n",
    "        if dm in skip_domains:\n",
    "            continue\n",
    "        clean_urls.write(url+'\\n')\n",
    "    url_file.close()\n",
    "    clean_urls.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywordExtractor(url):\n",
    "    url = url.strip()\n",
    "    headers={'User-Agent': 'Mozilla/5.0'}\n",
    "    resp = requests.get(url, headers=headers, timeout=3) \n",
    "    if resp.status_code >= 300:\n",
    "        return\n",
    "    html = resp.text\n",
    "    text = text_from_html(html)\n",
    "    title_text = text_from_title(html).lower()\n",
    "    max_ngram_size = 1\n",
    "    simple_kwextractor = yake.KeywordExtractor(lan=\"en\", n = max_ngram_size, dedupLim=0.9, dedupFunc='seqm', windowsSize=1, top=20, features=None)\n",
    "    keywords = simple_kwextractor.extract_keywords(text)\n",
    "    title_keywords = simple_kwextractor.extract_keywords(title_text)\n",
    "    title_words = dict([(t[1], t[0]) for t in title_keywords])  #to swap the term and its score to get {term:score}\n",
    "    title_words = title_words.keys()\n",
    "    \n",
    "    keywords2 = []\n",
    "    for kw in keywords:\n",
    "        #since the order of term and score is reversed\n",
    "        if kw[1] in title_words:\n",
    "            kw2 = (kw[0]/4, kw[1])\n",
    "        else:\n",
    "            kw2 = kw\n",
    "        keywords2.append(kw2)   #should be inside for loop so that all words are appended\n",
    "    keywords2 = dict([(t[1], t[0]) for t in keywords2]) #to make a dictionary(as given in description) and also to reverse the order to {term:score}\n",
    "    return keywords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load new history created since last update\n",
    "\n",
    "new_history = 'history.txt'\n",
    "cleaned_history = 'history_clean.txt'\n",
    "\n",
    "generate_clean_urls(new_history, cleaned_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords database structure\n",
    "\n",
    "keywords_db = {\n",
    "    'python' : {\n",
    "        'title'    : 1.0, \n",
    "        'body'     : 1.0, \n",
    "        'bookmark' : 1.0\n",
    "    },\n",
    "}\n",
    "\n",
    "#load keywords database from json file\n",
    "\n",
    "filename = \"keywords_db.data\"\n",
    "\n",
    "# try:\n",
    "#     with open(filename, 'rb') as f:\n",
    "#         keywords_db = pickle.load(f)\n",
    "# except:\n",
    "#     keywords_db = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new keywords that we need to add as nodes/add edges for \n",
    "# in our clustering graph.\n",
    "\n",
    "new_keywords_to_cluster = [['a', 'b'], ['c']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_new_history(history_url_file):\n",
    "    \n",
    "    history = open(history_url_file, 'r')\n",
    "    \n",
    "    for line in history.readlines():\n",
    "        # add code to get time spent on this webpage, scroll, etc.\n",
    "        # should be present in history file\n",
    "        try:\n",
    "            # taking 5 most significant keywords\n",
    "            keywords_list = keywordExtractor(line).items()\n",
    "            keywords_list = sorted(keywords_list, key=lambda key: key[1])\n",
    "            keywords_list = keywords_list[:5]\n",
    "            \n",
    "            # add keywords to be clustered\n",
    "            new_keywords_to_cluster.append(\n",
    "                [word for (word, value) in keywords_list])\n",
    "            \n",
    "            #for kw in keywords[:5]:\n",
    "                # code to update score of keyword\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    \n",
    "    history.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save keywords to cluster\n",
    "with open('to_cluster.data', 'wb') as outfile:\n",
    "    pickle.dump(new_keywords_to_cluster, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save keywords database\n",
    "with open('keyword_db.data', 'wb') as outfile:\n",
    "    pickle.dump(keywords_db, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
